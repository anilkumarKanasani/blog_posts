---
layout:
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: false
---

# ðŸ§‘âœˆ OpenAI API setup

## Installation

In this sample project, we will use the OpenAI API to interact with the ChatGPT model. We will write different types of prompts and analyse the responses given by the model. To use the API, we will need to install the [OpenAI Python package](https://pypi.org/project/openai/). This can be done by running the following command in your Jupyter notebook:

```
! pip install openai
```

## Get API Key

Once you have installed the OpenAI Python package in your local environment, you will need to obtain an API key to authenticate yourself to the OpenAI servers. You can generate a new API key from the OpenAI website: [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys). Please note that API usage is chargeable: [https://openai.com/pricing/](https://openai.com/pricing), so you will need to create an account and set up a billing plan before you can start using the API.

Once you have completed both of the above steps, you can use the following code to continue to write your prompts:

```
import openai
openai.api_key = "XX-XXX-XXXX-XXXX-XXXX"
```

## Type of API interactions

As per the openAI documentation, there are various ways to interact with API sever. some of the ways are as follows

1. [Chat Completions API](https://platform.openai.com/docs/guides/gpt/chat-completions-api)
2. [Function calling](https://platform.openai.com/docs/guides/gpt/function-calling)
3. [Completions API ](https://platform.openai.com/docs/guides/gpt/completions-api)

In this blog, we are using the Chat Completions API. It takes a list of messages as input and returns a single JSON object as output. Even though the API allows a list of messages as input, we can just send a single message to get a proper response from it.

### Input message object

In the below example message, we are using using **gpt-3.5-turbo** as our model to get predcitons.

1. The **system** message is useful to set the behaviour of the application. It is **optional**.
2. The **user** message block is the important and actual input from enduser. In this example, the end user is saying just **Hello** to the model.

The remaining parameters, such as **temperature, max\_tokens, top\_p, and penalty**, used to set instructions and to select a particular probability generated by the model.

* **Temperature** controls the randomness of the model's output
* **Max\_tokens** specifies the maximum number of tokens that the model should generate.
* **Top\_p** specifies the probability that a token should be generated.
* **Penalty** controls how much the model is penalised for generating certain tokens.

```python
openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
             {"role": "system", "content": "You are a helpful assistant."},
             {"role": "user","content": "Hello"}
           ],
  temperature=1,
  max_tokens=256,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0
)
```

### Response message object&#x20;

The response from the API has various different sub-components. A sample response object is as follows. The main useful response component is&#x20;

`choices[0].message["content"]`

The Model is responding back as  **Hello, How can I assist you ?**

```
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "Hello, How can I assist you ?",
        "role": "assistant"
      }
    }
  ],
  "created": 1677664795,
  "id": "chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW",
  "model": "gpt-3.5-turbo-0613",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 17,
    "prompt_tokens": 57,
    "total_tokens": 74
  }
}
```

The remaining sub-components are easy to understand based on their names.
