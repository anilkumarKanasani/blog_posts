---
cover: .gitbook/assets/Copy of Basics of Prompt Engineering.png
coverY: 5.020307082714215
layout:
  cover:
    visible: true
    size: full
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# ðŸš€ Introduction

{% hint style="info" %}
**Acknowledgement :**&#x20;

I would like to express my sincere gratitude to my instructors,  [Mike Chambers](https://www.linkedin.com/in/ACoAAAOBotwBjlI40oiyebPckX9cPG5K31TmJKc), [Antje Barth](https://www.linkedin.com/in/ACoAABl81U8BtWmVIZW5HeJ5XorSpuvUZjm\_Yyw), [Shelbee Eigenbrode](https://www.linkedin.com/in/ACoAAALlJFoBAOFBo-DGHYJ08UR9THOOaSOo-HE), [Chris Fregly](https://www.linkedin.com/in/ACoAAABTnPcBMVZJE2bhAShwKzrLwwUqN\_nIjFs) from [Amazon Web Services (AWS)](https://www.linkedin.com/company/amazon-web-services/) and [Andrew Ng](https://www.linkedin.com/in/andrewyng/) from [DeepLearning AI](https://www.deeplearning.ai/), for their guidance and support in helping me to learn about large language models. I gained most of the knowledge to write this blog post from their lectures and tutorials.

I would also like to thank the [Bard tool ](https://bard.google.com/)for generating the text for this blog post. I am grateful for the opportunity to use Bard and to learn about large language models.



I hope this blog post is informative and helpful.
{% endhint %}

### Generative AI Project Life Cycle Framework with LLMs

Large language models (LLMs) are a powerful new technology with the potential to revolutionize many industries. This blog post provides a comprehensive overview of the generative AI project life cycle framework, with a focus on LLMs.

The blog post begins by introducing the transformer architecture, which is the foundation of most modern LLMs. It then discusses in-context learning (also known as prompt engineering), which is a technique for fine-tuning LLMs to perform specific tasks. Next, the blog post covers instruction fine-tuning and parameter-efficient fine-tuning (PEFT), which are two other techniques for improving the performance of LLMs on specific tasks. Finally, the blog post discusses reinforcement learning from human feedback (RLHF), which is a technique for training LLMs to generate outputs that are more aligned with human preferences.

### Major Topics Covered

#### Basic Level

* The transformer architecture
* In-context learning (or) prompt engineering
* Full model training

#### Fine tunning

* Instruction fine-tuning
* Parameter-efficient fine-tuning (PEFT)

#### Human Feedback

* Reinforcement learning from human feedback (RLHF)
* Model Evaluation

#### Deployment & Inferencing

* Model optimizations for deployment
* Interacting with external applications
* Chain-of-thoughts
* Program-aided language models
* ReAct :Combining reasoning and action
*   LLM applicaiton architectures



