# K

<mark style="color:red;">**Kullbackâ€“Leibler divergence**</mark>&#x20;

<mark style="color:red;">**KL divergence**</mark> ( also called **relative entropy** and **I-divergence** ) is a type of statistical distance ( it is not a metrics, it is a distance ): a measure of how one probability distribution <mark style="color:red;">****</mark><mark style="color:red;">** **</mark>_<mark style="color:red;">**P**</mark>_ is different from a second/ reference probability distribution _<mark style="color:red;">**Q**</mark>_.

Interestingly is it not symmetric ( means KLD from P to Q is not same as from Q to P ).
